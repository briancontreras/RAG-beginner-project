{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal here is to create a simple RAG project in order to get a better understand of how RAG works, and how I can iterate past it. To create bigger scale projects. \n",
    "\n",
    " \n",
    "## Recollection of how Rag works.\n",
    "\n",
    "My understand of Rag is that there are documents that are processed in bits of text called chunks. These chunks are then embedded and stored in a 3d vector space. Once they are stored in the vector space they are indexed with a specifc value, this value will be used to retrieve the data again. \n",
    "\n",
    "Once we do all the embedding of the document we will then have the user pass in a query and this query will then be used to be compared against the documents and it will grab the k most related ones.\n",
    "\n",
    "Once gathered of text chunks it will be based into the context of the LLM and be used to generate a response.\n",
    "\n",
    "There are more advanced and less simple versions of this approuch that allows for more efficiency however we will be using the approuch for better understanding of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Gather the documents.\n",
    "\n",
    "For this example we will just be using 2 text files so we won't neccesarily have to load any data however documents might need to be loaded through api calls and other means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Clean the Data\n",
    "\n",
    "When we are working with text documents it's important to clean the data in order to avoid irrelevant or damaged text chunks.\n",
    "\n",
    "we do this with basic python string functions and the use of the regular expression library 're' to take out non-relevant ascii characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_data(text: str) -> str:\n",
    "    #This is going to sub the leading spaces in the file\n",
    "    text = re.sub(r'\\s+', '',text)\n",
    "\n",
    "    #This gets rid of all the nonASCII characters.\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    #This gets rid of any trailing white spaces.\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load  Text\n",
    "\n",
    "Now we will create a method to load the data of the documents one into an accessible array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#allows for us to read other directories.\n",
    "\n",
    "def load_data(folder_path):\n",
    "    #creates an array of documents that would match our txt files.\n",
    "    docs = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder_path,file),'r',encoding=\"utf-8\") as f:\n",
    "                docs.append(f.read())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In this type of machine learning (supervised), the model is trained on labeled data. \\nIn simple terms, every training example has an input and an associated output label. \\nThe objective is to build a model that generalizes well on unseen data. \\nCommon algorithms include:\\n- Linear Regression\\n- Decision Trees\\n- Random Forests\\n- Support Vector Machines\\n\\nClassification and regression tasks are performed in supervised machine learning.\\nFor example: spam detection (classification) and house price prediction (regression).\\nThey can be evaluated using accuracy, F1-score, precision, recall, or mean squared error.', 'In this type of machine learning (unsupervised), the model is trained on unlabeled data. \\nPopular algorithms include:\\n- K-Means\\n- Principal Component Analysis (PCA)\\n- Autoencoders\\n\\nThere are no predefined output labels; the algorithm automatically detects \\nunderlying patterns or structures within the data.\\nTypical use cases include anomaly detection, customer clustering, \\nand dimensionality reduction.\\nPerformance can be measured qualitatively or with metrics such as silhouette score \\nand reconstruction error.']\n"
     ]
    }
   ],
   "source": [
    "print(load_data(\"../tutorial-project/documents/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it loads all of the document and we are now ready to begin the cleaning portion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no imports since this is one notebook\n",
    "\n",
    "def prepare_data(folder_path=\"../tutorial-project/documents/\"):\n",
    "    rawData = load_data(folder_path)\n",
    "    cleanedData = [clean_data(doc) for doc in rawData]\n",
    "    print(f\"prepared {len(cleanedData)} documents\")\n",
    "\n",
    "    return cleanedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared 2 documents\n",
      "['Inthistypeofmachinelearning(supervised),themodelistrainedonlabeleddata.Insimpleterms,everytrainingexamplehasaninputandanassociatedoutputlabel.Theobjectiveistobuildamodelthatgeneralizeswellonunseendata.Commonalgorithmsinclude:-LinearRegression-DecisionTrees-RandomForests-SupportVectorMachinesClassificationandregressiontasksareperformedinsupervisedmachinelearning.Forexample:spamdetection(classification)andhousepriceprediction(regression).Theycanbeevaluatedusingaccuracy,F1-score,precision,recall,ormeansquarederror.', 'Inthistypeofmachinelearning(unsupervised),themodelistrainedonunlabeleddata.Popularalgorithmsinclude:-K-Means-PrincipalComponentAnalysis(PCA)-AutoencodersTherearenopredefinedoutputlabels;thealgorithmautomaticallydetectsunderlyingpatternsorstructureswithinthedata.Typicalusecasesincludeanomalydetection,customerclustering,anddimensionalityreduction.Performancecanbemeasuredqualitativelyorwithmetricssuchassilhouettescoreandreconstructionerror.']\n"
     ]
    }
   ],
   "source": [
    "print(prepare_data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see when running the code above that the process of preparing the data strips the code of the white spaces and the separates them into their own individual documents.\n",
    "\n",
    "with the data ready for use now, the next steps regard preparing for the retrieval process are chunking text, embedding, and then vectorizing. lets get started with chunking the text we have now. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Chunking the text.\n",
    "\n",
    "We will use a LANGCHAIN library to chunk the text into a size of 500 chunks with 100 chunk overlap.\n",
    "The 100 chunk overlap helps with retaining semantics in the text between chunks.\n",
    "\n",
    "We are now going to Pip install the Langchain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_text_splitters in c:\\users\\kobek\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain_text_splitters) (1.2.7)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.6.4)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.10.3)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (4.12.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.32.3)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_text_splitters) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents, chunk_size=500, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.create_documents(documents)\n",
    "    print(f\"Total chunks created {len(chunks)}\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Embedding Chunks\n",
    "\n",
    "Now that we have chunked text we can now embedd these text chunks to be vectorized for easy retrieval.\n",
    "\n",
    "First we are going to import the SentenceTransformer Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\kobek\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer #embed text chunks tool.\n",
    "import numpy as np #mulit dimensional array library\n",
    "\n",
    "def get_embeddings(text_chunks):\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    print(f\"embedding a total of {len(text_chunks)} chunks: \")\n",
    "\n",
    "    embeddings = model.encode(text_chunks,show_progress_bar=True)\n",
    "    print(f\"the shape of the embedding is {embeddings.shape} \")\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the embedding is regarding to the dimension of the vector that the embeddings are in.\n",
    "\n",
    "The model we used in this practice project is the 'sentence-transformer/all-MiniM-L6-v2' this project is good it allows for the embedding array to be 384 dimensions. \n",
    "\n",
    "These embedding dimesnions typically vary from 384 - 1024\n",
    "Larger dimensions are better for model performance, while smaller dimensions are better for faster computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Vectorize your embeddings\n",
    "\n",
    "we are going to use a facebook opensource library to vectorize the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\kobek\\anaconda3\\lib\\site-packages (1.13.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install faiss-cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this library we are going to create the vector and also return an index to the embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def build_faiss_index(embeddings, save_path=\"faiss_index\"):\n",
    "    dimensions = embeddings.shape[1]\n",
    "    print(f\"Building FAISS index with dimensions: {dimensions} \")\n",
    "\n",
    "    index = faiss.IndexFlatL2(dimensions)\n",
    "    index.add(embeddings.astype('float32'))\n",
    "\n",
    "    faiss.write_index(index, f\"{save_path}.index\")\n",
    "    print(f\"Saved faiss index to {save_path}.index\")\n",
    "\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create an index we are creating a datastructure that makes it easy for us to find specific embeddings.\n",
    "\n",
    "In this specific example it uses the IndexFlatL2 which means.\n",
    "\n",
    "Index -> Data Structure to look things up\n",
    "\n",
    "Flat -> No clustering, no Graphs\n",
    "\n",
    "L2 -> using euclidean distance, how close they are in space.\n",
    "\n",
    "\n",
    "#### Storing Metadata\n",
    "We are now going to store metadata that is the same index as the textchunks to be used for the retrieval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_metadata(text_chunks, path=\"faiss_metadata.pkl\"):\n",
    "    with open(path,\"wb\") as f:\n",
    "        pickle.dump(text_chunks,f)\n",
    "    print(f\"saved text metaData to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Retrieve Relevant information.\n",
    "\n",
    "so in order for us to deem information relevant we must first decide what we need to know. The way this works is by processing a user query, converting it to a numerical value, then compare these number to previously processed text chunks.\n",
    "\n",
    "This is called a Similarity Search.\n",
    "\n",
    "We need to load the vector (FAISS) and the metadata(Pkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_faiss_index(index_path=\"faiss_index.index\"):\n",
    "    print(\"loading FAISS index\")\n",
    "    return faiss.read_index(index_path)\n",
    "\n",
    "def load_metadata(metadata_path=\"faiss_metadata.pkl\"):\n",
    "    print(\"Loading MetaData\")\n",
    "    with open(metadata_path,\"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the metadata and the vector database how do we use them with the query to load the relevant chunks??!?!??!?!?!?\n",
    "\n",
    "We are going to use the same model we embedded the text chunks to embedd the query.\n",
    "\n",
    "Then use the index data structure to return the top_k most relevant chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_similar_chunks(query, index, text_chunks, top_k=3):\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    query_vector = model.encode([query]).astype('float32')\n",
    "\n",
    "    print(\"Searching for chunks....\")\n",
    "\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    print(f\"return {top_k} releven text chunks\")\n",
    "\n",
    "    return [text_chunks[i] for i in indices[0]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the relevant chunks we are going to combine it to a sinlge piece of context to pass into the LLM for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createContext(query,index,text_chunks,top_k=3):\n",
    "    context_chunks = retrieve_similar_chunks(query, index, text_chunks, top_k)\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate answer from LLM with context acquired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a small local model called TinyLlama, but since this usecase of rag is relevant to chatbots we can be fine implementing any other chatbot model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\kobek\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: requests in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kobek\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c0bf5db5004fb68fdff15cefb1be81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kobek\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kobek\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f782da88d154f13944175b704677738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "# Load at the top, outside generate_answer()\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.float16,  # use float16 if supported\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def generate_answer(query,top_k=3):\n",
    "    index = load_faiss_index()\n",
    "    text_chunks = load_metadata()\n",
    "    \n",
    "\n",
    "    context = createContext(query, index, text_chunks, top_k)\n",
    "    prompt = f\"\"\"\n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer: \n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    answer = full_text.split(\"Answer:\")[1].strip() if \"Answer:\" in full_text else full_text.strip()\n",
    "    print(\"\\nFinal Answer\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: create the Rag Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do is merge everything into one single linear process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    print(\"Load and Clean Data: \\n\")\n",
    "    documents = load_data(\"../tutorial-project/documents/\")\n",
    "    print(f\"Loaded {len(documents)} documents: \\n\")\n",
    "\n",
    "    print(\"Splitting text into chunks\\n\")\n",
    "    chunks_as_text = split_docs(documents, 500, 100,)\n",
    "\n",
    "    texts = [c.page_content for c in chunks_as_text]\n",
    "    print(f\"Created {len(texts)} text chunks. \\n\")\n",
    "\n",
    "    print(\"Generating embeddings: \\n\")\n",
    "    embeddings = get_embeddings(texts)\n",
    "\n",
    "    print(\"storing embeddings in vector space\\n\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "    store_metadata(texts)\n",
    "    print(\"Stored Embeddings and MetaData Successfully\\n\")\n",
    "\n",
    "    print(\"Retrieve similar chunks and generate answer\")\n",
    "    query = \"Does unsupervised ML cover regression tasks\"\n",
    "    generate_answer(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Clean Data: \n",
      "\n",
      "Loaded 2 documents: \n",
      "\n",
      "Splitting text into chunks\n",
      "\n",
      "Total chunks created 4\n",
      "Created 4 text chunks. \n",
      "\n",
      "Generating embeddings: \n",
      "\n",
      "embedding a total of 4 chunks: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3f25260ebf4725a83d73cf9f8c8e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the embedding is (4, 384) \n",
      "storing embeddings in vector space\n",
      "\n",
      "Building FAISS index with dimensions: 384 \n",
      "Saved faiss index to faiss_index.index\n",
      "saved text metaData to faiss_metadata.pkl\n",
      "Stored Embeddings and MetaData Successfully\n",
      "\n",
      "Retrieve similar chunks and generate answer\n",
      "loading FAISS index\n",
      "Loading MetaData\n",
      "Searching for chunks....\n",
      "return 3 releven text chunks\n",
      "\n",
      "Final Answer\n",
      "Yes, unsupervised ML can be used for regression tasks. \n",
      "For example, K-Means clustering can be used to group data points based on their similarity. \n",
      "The model can then predict the cluster membership of new data points. \n",
      "Autoencoders can be used to reconstruct the input data from its latent representation. \n",
      "These algorithms can be used for regression tasks such as clustering, dimensionality reduction, and feature selection.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
